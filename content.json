{"meta":{"title":"Little's Blogs","subtitle":"人生里有些事情，不能蹉跎。","description":null,"author":"Little","url":"https://chengaf.github.io/afcheng.github.io"},"pages":[],"posts":[{"title":"gcn","slug":"gcn","date":"2019-03-04T12:20:41.000Z","updated":"2019-03-04T12:23:39.132Z","comments":true,"path":"2019/03/04/gcn/","link":"","permalink":"https://chengaf.github.io/afcheng.github.io/2019/03/04/gcn/","excerpt":"","text":"Semi-Supervised Classification with Graph Convolutional Networks Info 2018 ICLR, Thomas N. Kipf (University of Amsterdam), Max Welling Contribute The author proposed a specific graph-based neural network model f(X, A) with convolution operator: \\hat{D}^{-\\frac{1}{2}}\\hat{A}\\hat{D}^{-\\frac{1}{2}}where: \\hat{A} = A+I_{N} is the adjacency matrix of undirected graph \\mathcal{G} with added self-connections. I_{N} is the identity matrix. \\hat{D}_{ii}=\\sum_{j}\\hat{A}_{ij} Layer-wise propagation rule is: (每一层的卷积计算规则) H^{(l+1)}=\\sigma(\\hat{D}^{-\\frac{1}{2}}\\hat{A}\\hat{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)})where: W^{l} is a layer-specific trainable weight matrix, \\sigma(\\cdot) denotes an activation function, such as the ReLU(\\cdot)=max(0,\\cdot) , H^{(l)}\\in \\mathbb{R}^{N\\times D} is the matrix of activations in the l^{th} layer; H^{(0)}=X . Author showed this form of propagation rule can be motivated via a first-order approximation of localized spectral filters on graph. Semi-supervised node classification Prove the effectiveness of f(X, A) for information propagation on graphs. Other 传统卷积无法处理graph这种non euclidean structure数据. (可理解，与每个节点相邻的节点个数不相等) Why \\hat{D}^{-\\frac{1}{2}}\\hat{A}\\hat{D}^{-\\frac{1}{2}} ? Feature extract in Fourier domain: g_{\\theta}\\star x=Ug_{\\theta}U^{T}x where: L=I_{N}-D^{-1/2}AD^{-1/2}=U\\Lambda U^{T} (L is the normalized graph Laplacian), \\Lambda is a diagonal matrix of eigenvalues and U​ is the matrix of eigenvectors, g_{\\theta} is the function of \\Lambda​. 下图帮助理解 时域卷积等于频域乘积，时域乘积等于频域卷积","categories":[],"tags":[]},{"title":"PageRank and RWR","slug":"PageRank-and-RWR","date":"2018-12-29T07:11:51.000Z","updated":"2019-01-07T01:46:10.074Z","comments":true,"path":"2018/12/29/PageRank-and-RWR/","link":"","permalink":"https://chengaf.github.io/afcheng.github.io/2018/12/29/PageRank-and-RWR/","excerpt":"","text":"写在前面a. 看到一篇paper里用RWR (random walk with restart)，得到网络Network中每个节点的初始化特征。b. 单看RWR公式和PageRank略相似，再过一遍再理解c. 这里的记录只针对我关注的（个人笔记），必定片面，想对相关技术做全面了解这里不合适，望别被我带偏 符号化表示1, N 个网页（Network中N个节点），2, 归一化的网页间链接关系（节点间的邻接关系）A\\in R^{N*N}(可以类似马氏链的状态转移矩阵)。 PageRank 最原始PageRank要得到网页的重要性(被用户访问到的可能性) 用向量 B=(b_{1},b_{2},...,{b_{N}})^{T} 表示。B 可根据如下公式计算: B_{t}=B_{t-1}\\ A \\ \\ \\ 公式(1)t 时刻的网页重要性B_{t} 取决于t-1 时刻的 B_{t-1} 以及链接关系A 。如果B_{t}=B_{t-1}A=B_{t-1} ,则此时的 B_{t+1}=B_{t}=B_{t-1} 称为马尔科夫链的平稳分布(stationary distribution)，PageRank需要的网页被用户看到的概率就是这个平稳分布。 Personalized PageRank 考虑到将用户个人偏好(用户肯定是想访问啥访问啥)，所以在公式(1)基础上除了考虑链接关系得再加一项考虑以一定概率随机访问其它所有页面，公式化描述如下： B_{t} = c\\ B_{t-1}\\ A+(1-c)\\ p\\ \\ \\ 公式(2)其中，1-c 表示用户不按照链接关系随机访问其它页面的概率，p=(\\frac{1}{N},\\frac{1}{N},...)^{T} 表示均匀的访问其它页面。 Random Walk with Restart Random Walk的游走规则公式(1)","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2018-12-29T02:23:07.772Z","updated":"2018-12-29T06:37:36.471Z","comments":true,"path":"2018/12/29/hello-world/","link":"","permalink":"https://chengaf.github.io/afcheng.github.io/2018/12/29/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Starty=wx_{1}+bCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}